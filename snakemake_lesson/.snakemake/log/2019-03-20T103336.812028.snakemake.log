Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 6
Rules claiming more threads will be scaled down.
Provided resources: memory=12
Job counts:
	count	jobs
	1	all
	4	create_partials
	5

[Wed Mar 20 10:33:36 2019]
rule create_partials:
    output: partial_3.txt
    jobid: 4
    wildcards: number=3
    resources: memory=6

[Wed Mar 20 10:33:36 2019]
rule create_partials:
    output: partial_2.txt
    jobid: 3
    wildcards: number=2
    resources: memory=6

[Wed Mar 20 10:33:37 2019]
Finished job 4.
1 of 5 steps (20%) done
[Wed Mar 20 10:33:37 2019]
Finished job 3.
2 of 5 steps (40%) done

[Wed Mar 20 10:33:37 2019]
rule create_partials:
    output: partial_0.txt
    jobid: 1
    wildcards: number=0
    resources: memory=6

[Wed Mar 20 10:33:37 2019]
rule create_partials:
    output: partial_1.txt
    jobid: 2
    wildcards: number=1
    resources: memory=6

[Wed Mar 20 10:33:37 2019]
Finished job 1.
3 of 5 steps (60%) done
[Wed Mar 20 10:33:37 2019]
Finished job 2.
4 of 5 steps (80%) done

[Wed Mar 20 10:33:37 2019]
rule all:
    input: partial_0.txt, partial_1.txt, partial_2.txt, partial_3.txt
    output: result.txt
    jobid: 0

[Wed Mar 20 10:33:37 2019]
Finished job 0.
5 of 5 steps (100%) done
Complete log: /home/alessandro/Python/snakemake_lesson/.snakemake/log/2019-03-20T103336.812028.snakemake.log

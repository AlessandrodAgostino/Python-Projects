\relax 
\@writefile{toc}{\contentsline {paragraph}{Prior Penalized regression}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The three plots above show the three coefficients sets obtained through the three regression method on the whole dataset after the MinMax regularization. The values have been sorted by absolute value. The \leavevmode {\color  {red}red} vertical line shows the cut over the first 50 features. The \leavevmode {\color  {ForestGreen}green} horizontal line detect the value of the 50$^{th}$ coefficient. In all of the three curves it's evident the \emph  {elbow} behave of the curve and that the \emph  {TOP 50} features cut falls around the elbow point.\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:3_coef}{{1}{2}}
\@writefile{toc}{\contentsline {paragraph}{Dimensionality Reduction}{2}}
\@writefile{toc}{\contentsline {paragraph}{Example - \emph  {Brain Challenge}}{3}}
\@writefile{toc}{\contentsline {paragraph}{The Dataset}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Series of violin graphs that show the distribution in gender and age among all the site of analysis. It turned out a clear majority of young surveyed of age around 20 years respect to the more grown, and only 3 or 4 sites cover homogeneously a wide range of age. The distribution between male and female instead appears to be quite symmetrical for each site. \relax }}{4}}
\newlabel{fig:gend_distr}{{2}{4}}
\@writefile{toc}{\contentsline {paragraph}{Dimensionality Reduction}{4}}
\@writefile{toc}{\contentsline {paragraph}{Regression on Reduced Dataset}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces These are the three $R^2$ score obtained training the SVR on three different reductions of the feature space: selecting only those features that fell at least once in the \emph  {TOP 50}, \emph  {TOP 25} and \emph  {TOP 10}. It's easy to see that reducing the dimension of the space the $R^2$ value drops. It's interesting seeing how a reduction in dimension from 954 to 98 (an order of magnitude) could anyway provide such a good $R^2$ score, around 0.74.\relax }}{5}}
\newlabel{fig:Brain_graphs}{{3}{5}}
\@writefile{toc}{\contentsline {paragraph}{The Dataset}{5}}
\@writefile{toc}{\contentsline {paragraph}{Dimensionality Reduction}{5}}
\citation{doe}
\@writefile{toc}{\contentsline {paragraph}{Regression on Reduced Dataset}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces These are the three $R^2$ score obtained training the GPR on four different reductions of the feature space: selecting the whole dataset and those features that fell at least once in the \emph  {TOP 20}, \emph  {TOP 10} and \emph  {TOP 5}. It's easy to see that reducing the dimension of the space the $R^2$ value drops. It's interesting to see how the reduction in dimension from the whole dataset (72 features) to the 23 features in the \emph  {TOP 20} slightly reduces the score from 0.35 to 0.31. A further reduction instead sharply reduces the score.\relax }}{6}}
\newlabel{fig:Cardio_graphs}{{4}{6}}
\newlabel{sec:app1}{{}{7}}
\bibcite{doe}{Doe}
\newlabel{sec:app2}{{}{8}}
